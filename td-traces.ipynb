{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from long_nardy import LongNardy\n",
    "from state import State\n",
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_layers = []\n",
    "        hidden_layers.append(nn.Linear(100, 40))\n",
    "        hidden_layers.append(nn.ReLU())\n",
    "        for _ in range(80):\n",
    "            hidden_layers.append(nn.Linear(40, 40))\n",
    "            hidden_layers.append(nn.ReLU())\n",
    "        hidden_layers.append(nn.Linear(40, 1))\n",
    "        hidden_layers.append(nn.Sigmoid())\n",
    "\n",
    "        self.net = nn.Sequential(*hidden_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, lr, gamma, epsilon, lambda_):\n",
    "        super().__init__()\n",
    "        self.net = ANN()\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.lambda_ = lambda_\n",
    "        self.eligibility_traces = {name: torch.zeros_like(param).to(device) for name, param in self.net.named_parameters()}\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=lr)\n",
    "\n",
    "    def get_value(self, state):\n",
    "        \"\"\"Predict V(s) for a state tensor.\"\"\"\n",
    "        state_tensor = torch.tensor(state.get_tensor_representation(), dtype=torch.float32, requires_grad=True).to(device)\n",
    "        return self.net(state_tensor)\n",
    "\n",
    "    def update_eligibility_traces(self):\n",
    "        \"\"\"Decay and reset eligibility traces.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            for name in self.eligibility_traces:\n",
    "                self.eligibility_traces[name] *= (self.gamma * self.lambda_)\n",
    "\n",
    "    def reset_eligibility_traces(self):\n",
    "        \"\"\"Reset traces to zero.\"\"\"\n",
    "        for name in self.eligibility_traces:\n",
    "            self.eligibility_traces[name].zero_()\n",
    "\n",
    "    def epsilon_greedy(self, candidate_states: List[State]):\n",
    "        \"\"\"Select a state from candidates using ε-greedy policy.\"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Random exploration\n",
    "            chosen_idx = np.random.randint(len(candidate_states))\n",
    "            value = self.get_value(candidate_states[chosen_idx])\n",
    "        else:\n",
    "            values = [self.get_value(state) for state in candidate_states]\n",
    "            values_float = [value.item() for value in values]\n",
    "            chosen_idx = np.argmax(values_float)\n",
    "            value = values[chosen_idx]\n",
    "        return candidate_states[chosen_idx], value\n",
    "    \n",
    "    def update_weights(self, td_error):\n",
    "        \"\"\"Update network weights using eligibility traces and TD error.\"\"\"\n",
    "        for name, param in self.net.named_parameters():\n",
    "            param.data += self.optimizer.param_groups[0]['lr'] * td_error * self.eligibility_traces[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = LongNardy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = Agent(0.9, 0.9, 0.9, 0.9).to(device)\n",
    "agent2 = Agent(0.9, 0.9, 0.9, 0.9).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn 1\n",
      "getting states\n",
      "got 4 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 2\n",
      "getting states\n",
      "got 11 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 3\n",
      "getting states\n",
      "got 9 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 4\n",
      "getting states\n",
      "got 15 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 5\n",
      "getting states\n",
      "got 22 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 6\n",
      "getting states\n",
      "got 33 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 7\n",
      "getting states\n",
      "got 32 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 8\n",
      "getting states\n",
      "got 33 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 9\n",
      "getting states\n",
      "got 19 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 10\n",
      "getting states\n",
      "got 40 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 11\n",
      "getting states\n",
      "got 27 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 12\n",
      "getting states\n",
      "got 32 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 13\n",
      "getting states\n",
      "got 55 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 14\n",
      "getting states\n",
      "got 58 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 15\n",
      "getting states\n",
      "got 61 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 16\n",
      "getting states\n",
      "got 40 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 17\n",
      "getting states\n",
      "got 24 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 18\n",
      "getting states\n",
      "got 33 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 19\n",
      "getting states\n",
      "got 37 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 20\n",
      "getting states\n",
      "got 30 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 21\n",
      "getting states\n",
      "got 27 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 22\n",
      "getting states\n",
      "got 62 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 23\n",
      "getting states\n",
      "got 17 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 24\n",
      "getting states\n",
      "got 62 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 25\n",
      "getting states\n",
      "got 888 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 26\n",
      "getting states\n",
      "got 44 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 27\n",
      "getting states\n",
      "got 34 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 28\n",
      "getting states\n",
      "got 37 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 29\n",
      "getting states\n",
      "got 7 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 30\n",
      "getting states\n",
      "got 57 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 31\n",
      "getting states\n",
      "got 16 states\n",
      "calculating next values\n",
      "calculated next values\n",
      "zeroing grad\n",
      "backprop\n",
      "Turn 32\n",
      "getting states\n",
      "got 42264 states\n",
      "STOP\n",
      "Episode 2\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 1\n",
    "state_generator = game\n",
    "for episode in range(num_episodes):\n",
    "    # Initialize episode\n",
    "    candidate_states = state_generator.get_states_after_dice()\n",
    "    done = False\n",
    "    agent_turn = 0  # Alternate turns between agents (0: agent1, 1: agent2)\n",
    "\n",
    "    i = 0\n",
    "    while not done:\n",
    "        # state_generator.state.pretty_print()\n",
    "        i += 1\n",
    "        print(f\"Turn {i}\")\n",
    "        # Select agent based on turn\n",
    "        agent = agent1 if agent_turn == 0 else agent2\n",
    "\n",
    "        # 1. Select state using ε-greedy\n",
    "        chosen_state, current_value = agent.epsilon_greedy(candidate_states)\n",
    "        \n",
    "        # 2. Observe reward and next states\n",
    "        state_generator.step(chosen_state)\n",
    "\n",
    "        if state_generator.is_finished():\n",
    "            reward = 1\n",
    "            next_value = 0.0\n",
    "            done = True\n",
    "        else:\n",
    "            reward = 0\n",
    "            print(\"getting states\")\n",
    "            next_candidate_states = state_generator.get_states_after_dice()\n",
    "            print(f\"got {len(next_candidate_states)} states\")\n",
    "            if (len(next_candidate_states) > 10000):\n",
    "                print(\"STOP\")\n",
    "                episode = num_episodes\n",
    "                error_state = state_generator.state.copy()\n",
    "                error_states = next_candidate_states\n",
    "                break\n",
    "\n",
    "            if len(next_candidate_states) == 0:\n",
    "                state_generator.step(chosen_state)\n",
    "                continue\n",
    "            \n",
    "            # 3. Compute TD error\n",
    "            with torch.no_grad():\n",
    "                print(\"calculating next values\")\n",
    "                next_values = [agent.get_value(ns).item() for ns in next_candidate_states]\n",
    "                print(\"calculated next values\")\n",
    "                next_value = max(next_values)\n",
    "\n",
    "        td_error = reward + agent.gamma * next_value - current_value.item()\n",
    "\n",
    "        # 4. Compute gradients and update eligibility traces\n",
    "        print(\"zeroing grad\")\n",
    "        agent.net.zero_grad()\n",
    "        print(\"backprop\")\n",
    "        current_value.backward()\n",
    "        agent.update_eligibility_traces()\n",
    "        for name, param in agent.net.named_parameters():\n",
    "            agent.eligibility_traces[name] += param.grad\n",
    "\n",
    "        # 5. Update weights\n",
    "        agent.update_weights(td_error)\n",
    "\n",
    "        # 6. Prepare for next step\n",
    "        candidate_states = next_candidate_states\n",
    "\n",
    "        # Switch turns\n",
    "        agent_turn = 1 - agent_turn\n",
    "\n",
    "    if episode < 10:\n",
    "        print(f\"Episode {episode+1}\")\n",
    "    elif episode < 100 and episode % 10 == 0:\n",
    "        print(f\"Episode {episode+1}\")\n",
    "    elif episode < 1000 and episode % 100:\n",
    "        print(f\"Episode {episode+1}\")\n",
    "    elif episode % 1000:\n",
    "        print(f\"Episode {episode+1}\")\n",
    "\n",
    "        torch.save(agent1.state_dict(), f\"agent1_epoch_{episode}_latest.pth\")\n",
    "        torch.save(agent2.state_dict(), f\"agent2_epoch_{episode}_latest.pth\")\n",
    "game.state.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 2, 2]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_state.dice_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nardy Board State:\n",
      "-------------------------------------------------\n",
      "12 [-1]  13 [ 1]  14 [ 2]  15 [ 1]  16 [-1]  17 [ 2]  18 [ .]  19 [ 1]  20 [-2]  21 [ .]  22 [ 1]  23 [ 3]  \n",
      "-------------------------------------------------\n",
      "11 [-3]  10 [ .]   9 [ 1]   8 [-1]   7 [ .]   6 [-4]   5 [ 1]   4 [ .]   3 [ 1]   2 [-2]   1 [ 1]   0 [-1]  \n",
      "-------------------------------------------------\n",
      "\n",
      "White Turn:  True\n",
      "Dice:  [2, 2, 2, 2]\n",
      "White off:  0\n",
      "Black off:  0\n"
     ]
    }
   ],
   "source": [
    "error_state.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = LongNardy()\n",
    "test.state = error_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_states = test.get_states_after_dice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open(\"error_state.json\", \"w\") as f:\n",
    "#     json.dump(error_state.to_dict(), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
